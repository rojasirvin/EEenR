---
title: "Máxima verosimilitud"
author: "Irvin Rojas"
date: "20 de febrero de 2021"
header-includes:
  - \usepackage{tikz}
  - \usetikzlibrary{shapes, shadows,arrows}
output:
  xaringan::moon_reader:
    css: [default, "libs/cide.css", metropolis-fonts, "https://stackpath.bootstrapcdn.com/bootstrap/4.3.1/css/bootstrap-grid.min.css", "https://use.fontawesome.com/releases/v5.7.2/css/all.css", "https://cdn.rawgit.com/jpswalsh/academicons/master/css/academicons.min.css"]
    seal: false
    chakra: "https://remarkjs.com/downloads/remark-latest.min.js"
    lib_dir: libs
    nature:
      highlightStyle: github
      highlightLines: true
      countIncrementalSlides: false
      titleSlideClass: ["middle", "center"]
      ratio: "16:9"
      beforeInit: ["https://platform.twitter.com/widgets.js", "libs/cols_macro.js"]
      navigation:
        scroll: false
---

class: title-slide

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, fig.path = "figures/")
library(tidyverse)
library(magick)
library(reticulate)
xfun::pkg_load2(c('base64enc', 'htmltools', 'mime'))
```

.title[
# Sesión 12. Máxima verosimilitud
]

.subtitle[
## Estadística y Econometría en R
]

.author[
### Irvin Rojas <br> [rojasirvin.com](https://www.rojasirvin.com/) <br> [<i class="fab fa-github"></i>](https://github.com/rojasirvin) [<i class="fab fa-twitter"></i>](https://twitter.com/RojasIrvin) [<i class="ai ai-google-scholar"></i>](https://scholar.google.com/citations?user=FUwdSTMAAAAJ&hl=en)
]

---
# Agenda
  
1. Estudiaremos el problema de máxima verosimilitud

1. Analizaremos el problema de la estimación de probabilidades no lineales

1. Implementaremos modelos probit y logit en R

1. Interpretaremos los resultados de la estimación de modelos de probabilidad no lineal

---

# Estimadores no lineales

Los estimadores no lineales son funciones no lineales de la variable dependiente

Pueden surgir por:
   - Variable dependiente categórica o de conteo
   
   - Censura o truncamiento
   
   - Selección de la muestra

En este curso nos enfocaremos en los modelos de variable dependiente binaria y los modelos de conteo

---

# Principio de máxima verosimilitud

La función de masa de probabilidad o densidad $f(y,X|\theta)$ es una función del parámetro $\theta$ y los datos $(y,X)$

A esto se le llama **función de verosimilitud** y frecuentemente se le denota $L_N(\theta|y,X)$

El problema de máxima verosimilitud consiste en estimar el vector de parámetros para $\theta_0$ que maximice la probabilidad de observar los datos

Maximizar $L_N$ es equivalente a maximizar  $\mathcal{L}_N(\theta)=\ln(L_N(\theta))$ 

Cuando trabajamos con secciones cruzadas con observaciones independientes, $f(y|X,\theta)=\Pi_i f(y_i|x_i,\theta)$

Y entonces, la función de log verosimilitud se define como:

$$Q_N(\theta)=N^{-1}\mathcal{L}(\theta)=N^{-1}\sum_i\ln f(y_i|x_i,\theta)$$
---

# Estimador de máxima verosimilitud

El **estimador de máxima verosimilitud** es el estimador que maximiza la función de log verosimilitud

Formalmente, se conoce como el **estimador de maxima verosimilitud condicional** al máximo local que satisface la condición de primer orden:

$$\frac{1}{N}\frac{\partial \mathcal{L}_N(\theta)}{\partial \theta }=\frac{1}{N}\sum_i\frac{\partial \ln f(y_i|x_i,\theta)}{\partial \theta}=0$$

El adjetivo de **condicional** se debe a que el estimador se basas en la densidad de $y$ dado $x$, pero comúnmente se emplea solo el término de estimador de máxima verosimilitud

Al vector gradiente de primeras derivadas parciales $s(\theta)=\frac{\mathcal{L}_N(\theta)}{\partial \theta}$ se le conoce como **vector score**

Al score evaluado en $\theta_0$ se le conoce como **score eficiente**

---

# Estimador de máxima verosimilitud

El estimador de máxima verosimilitud es comúnmente usado en econometría

Es el estimador más eficiente de entre los estimadores asintóticamente normales

El principio de verosimilitud (Fisher, 1922) consiste en escoger el vector de parámetros que maximice la probabilidad de observar los datos

En este contexto, consideramos a la función de masa de probabilidad o densidad $f(y,X|\theta)$ como una función de $\theta$, dados unos datos $(y,X)$

Recordemos que el **estimador de máxima verosimilitud condicional** al máximo local que satisface la condición de primer orden:

$$\frac{1}{N}\frac{\partial \mathcal{L}_N(\theta)}{\partial \theta }=\frac{1}{N}\sum_i\frac{\partial \ln f(y_i|x_i,\theta)}{\partial \theta}=0$$
---

# Ejemplo Bernoulli

Una de las aplicaciones que veremos más adelante trata el problema de modelos de probabilidad no lineal

Consideremos un evento que tiene probabilidad de ocurrir $p$ y de no ocurrir $1-p$

$$y=\cases{1\quad \text{con probabilidad}\quad p \\ 0 \quad \text{con probabilidad}\quad 1-p}$$

Podemos escribir la función de probabilidad como

$$f(y_i)=p_i^{y_i}(1-p_i)^{1-y_i}$$
Usamos una serie de características $x_i$ para modelar las probabilidades:

$$p_i=F(x_i'\beta)$$
Y la función de verosimilitud de la $i$-ésima observación es

$$f(y_i|x_i,\beta)=F(x_i'\beta)^{y_i}(1-F(x_i'\beta))^{1-y_i}$$
---

# Ejemplo Bernoulli

El problema de máxima verosimilitud con una muestra $\{(y_i,x_i\}$ con $N$ datos consiste en encontrar $\beta$ que maximice la función de log verosimilitud

La función de verosimilitud es la densidad conjunta

--

Bajo independencia, la densidad conjunta es $\Pi_i f(y_i|x_i,\beta)$

Y la función de log verosimilitud es el log del producto, es decir, la suma de los logs: $\sum_i \ln f(y_i|x_i,\beta)$

En nuestro ejemplo Bernoulli, la log densidad para la observación $i$ es:

$$\mathcal{l}_i(y_i|x_i,\beta) =\ln f(y_i|x_i,\beta)=y_i \ln(F(x_i'\beta))+(1-y_i)\ln(1-F(x_i'\beta))$$
---

# El estimador de MV en el caso Bernoulli

El estimador $\hat{\beta}_{MV}$ maximiza la función objetivo

$$Q_N(\beta)=\frac{1}{N}\sum_i \left(y_i \ln(F(x_i'\beta))+(1-y_i)\ln(1-F(x_i'\beta))\right)$$

La condición de primer orden es:

$$\frac{1}{N}\sum_i\left(\frac{y_i}{F(x_i'\beta)}F'(x_i'\beta)x_i-\frac{1-y_i}{1-F(x_i'\beta)}F'(x_i'\beta)x_i\right)\Bigg|_{\hat{\beta}}=0$$

Noten que si $F(\cdot)$ es la función acumulativa, entonces $F'(\cdot)=f(\cdot)$ es la función de densidad

Esta expresión no tiene una solución cerrada y usamos métodos numéricos para calcular $\hat{\beta}$

Se buscan los valores de $\beta$ que hagan que la condición de primer orden se cumpla (hasta cierto nivel de tolerancia)

---

class: inverse, middle, center

# Distribución del estimador de MV


---

# Distribución del estimador de MV

Supongamos:

1. El pgd es la densidad condicional $f(y_i|x_i,\theta)$ usada para definir la función de verosimilitud

1. La función de densidad $f(\cdot)$ satisface $f(y,\theta^{(1)})=f(y,\theta^{(2)})$ si y solo si $\theta^{(1)}=\theta^{(2)}$

1. La matriz $A_0=p\lim\frac{1}{N}\frac{\partial^2\mathcal{L}_N(\theta)}{\partial \theta \partial\theta'}\Bigg|_{\theta_0}$ existe y es no singular

1. El orden de la diferenciación e integración de la función de log verosimilitud puede ser invertido
  
--

Entonces el estimador de MV, definido como la solución a las condiciones de primer orden, es consistente para $\theta_0$ y

$$\sqrt{N}(\hat{\theta}_{MV}-\theta_0)\xrightarrow{d}\mathcal{N}(0,-A_0^{-1})$$
---

# Distribución del estimador de MV

La condición clave es 1, es decir, que el modelo está correctamente especificado

La condición 2 es técnica e implica identificación

La condición 3 es parecida a lo que asumiamos en MCO para poder aplicar una LGN al promedio $N^{-1}X'X$

La mayoría de nuestras aplicaciones satisfacen el supuesto 4. Este supuesto excluye a las distribuciones cuyos rangos depende de $\theta$, como la uniforme


---

class: inverse, middle, center


# Modelos de probabilidad no lineal

---

# Variable dependiente binaria

$y_i$ toma el valor de 1 si el evento se realiza y 0 si no

Los datos siguen una distribución Bernoulli con probabilidad que varía entre individuos: $p\equiv p_i$

Especificamos una forma funcional para la probabilidad y se estima por MV

---

# Modelo general

La variable dependiente:
$$
y_i=
\begin{cases}
1 \quad\text{con probabilidad}\quad p \\
0 \quad\text{con probabilidad}\quad 1-p
\end{cases}
$$
Parametrizamos $p_i$ con un vector de características $x_i$ y un vector de parámetros $\beta$:

$$p_i=F(y_i=1|x_i)=F(x_i'\beta)$$
- A $x_i'\beta$ se le conoce como *índice*, por lo que este modelo es también un modelo de un índice único

--

$F$ es una función de distribución acumulada (cdf)

Un modelo de probabilidad lineal simplemente especifica $p_i=x_i'\beta$

---

# Probit y logit

Un modelo probit especifica $F\cdot$ como una normal estándar con cdf dada por:

$$\Phi(x'\beta)=\int_{-\infty}^{\infty}\phi(z)dz$$

--

Un modelo logit especifica a $F\cdot$ como una función logística:

$$\Lambda(x'\beta)=\frac{exp\{x'\beta\}}{1+exp\{x'\beta\}}$$

---

# Efectos marginales

En un modelo lineal, $\beta_j$ tiene la interpretación directa del efecto de un cambio marginal en $x_j$ sobre $y$

En cambio, en los modelos de probabilidad no lineal estamos interesaods en:

$$\frac{\partial P(y_i=1)|x_i)}{\partial x_{ij}}=F'(x_i'\beta)\beta_j$$

Como $F(\cdot)$ es no lineal, los efectos marginales difieren del punto de evaluación, es decir, de $x_i'\beta$

--

En el caso probit:

$$\frac{\partial P(y_i=1)|x_i)}{\partial x_{ij}}=\phi(x'\beta)\beta_j$$

--

En el caso logit:

$$\frac{\partial P(y_i=1)|x_i)}{\partial x_{ij}}=\Lambda(x'\beta)(1-\Lambda(x'\beta))\beta_j$$

---

# Efectos marginales

Dos efectos marginales que podemos calcular:

1. Promedio de efectos marginales (más usado)

    $$\frac{1}{N}\sum_i F'(x_i'\hat{\beta})\hat{\beta}_j$$

1. Efecto marginal evaluado en la media de $x$ (menos usado)

    $$F'(\bar{x}'\hat{\beta})\hat{\beta}_j$$

--

Noten que el cociente de efectos marginales es igual al cociente de los coeficientes estimados:

$$\frac{\frac{\partial P(y_i=1)|x_i)}{\partial x_{ij}}}{\frac{\partial P(y_i=1)|x_i)}{\partial x_{ik}}}=\frac{\hat{\beta}_j}{\hat{\beta}_k}$$

---

# Estimación

Tenemos a la mano datos $(y_i,x_i)$ de $N$ individuos

La función de masa de probabilidad para $y_i$ es:

$$f(y_i|x_i)=p_i^{y_i}(1-p_i)^{1-y_i},\quad\quad y_i={0,1}$$

Recordemos que $p_i=F(x_i'\beta)$

---

# Estimación

La función de log verosimilitud es


$$\mathcal{L}(\beta)=\sum_i\{y_i\ln F(x_i'\beta) + (1-y_i)\ln(1-F(x_i'\beta))\}$$
La condición de primer orden implica que $\hat{\beta}_{MV}$ resuelve:

$$\sum_i \left(\frac{y_i-F(x_i'\beta)}{F(x_i'\beta)(1-F(x_i'\beta))}F'(x_i'\beta)x_i\right)=0$$

---

# Particularidades del modelo logit

Una medida comúnmente usada es la razón de momios u *odds ratio*, también llamado riesgo relativo: $\frac{p}{1-p}$

El riesgo relativo es la probabilidad de que suceda $y=1$ relativa a la probabilidad de que $y=0$

En el caso del logit, el riesgo relativo es:

$$\frac{p}{1-p}=exp\{x'\beta\}$$

Y el log del riesgo relativo es simplemente:

$$\ln\left(\frac{p}{1-p}\right)=x'\beta$$

Es decir, el log del riesgo relativo o el log de la razón de momios es lineal en $x$

---

# Particularidades del modelo logit

Noten que expresar las probabilidades como riesgo relativo tiene una interpretación usada comúnmente en bioestadística

Si $\frac{p}{1-p}=exp\{x'\beta\}$ y $x_j$ cambia en una unidad, entonces el lado derecho se vuelve $exp\{x'\beta+\beta_j\}=exp\{x'\beta\} exp\{\beta_j\}$

Es decir, el riesgo relativo se ha incrementado $exp\{\beta_j\}$ veces

Supongamos que $\hat{\beta}_j=0.05$, entonces $exp\{0.05\}\approx 1.05$

--

Es decir, el riesgo relativo se incrementa en aproximadamente 5%

---

# ¿Probit o logit?

Empíricamente suelen desempeñarse de forma muy similar

Como nos interesan los efectos marginales, la diferencia entre los modelos usados suele ser mínima

El modelo logit es frecuentemente usado en bioestadística por su interpretación en términos de riesgo relativo

El probit se puede motivar por un modelo de variable latente normal, que se liga directamente al model Tobit (que veremos más adelante)

---

class: inverse, center, middle

# Ejemplo con los datos de covid-19

---

# Datos de covid-19

Usamos una base con los datos individuales de pacientes covid-19

Construyo algunas variables dummy para hospitalizados, pacientes en UCI, pacientes con obesidad y que fuman

Me quedo con 5% de los datos

```{r tidy=T, echo=TRUE, message=F, results='hide', message=F, warning=F}
setwd("C:/Users/Irvin/Dropbox/Ejercicios en R/")

data.covid <- read_csv(file = "./datos/covid_limpia_210203.csv",
                    locale = locale(encoding = "latin1"))

data.covid <- data.covid %>% 
  mutate(uci=ifelse(uci=="Sí",1,0)) %>% 
  mutate(fuma=ifelse(tabaquismo=="Sí",1,0)) %>% 
  mutate(hospital=ifelse(tipo_paciente=="Hospitalizado",1,0)) %>% 
  mutate(obesidad=ifelse(obesidad=="Sí",1,0))

set.seed(221)
data.covid <- slice_sample(data.covid, prop=.05)

```

Estos datos actualizan diario [acá](https://datos.covid-19.conacyt.mx/#DownZCSV)

---
# Datos de covid-19

Exploramos un poco los datos

```{r tidy=T, echo=T, include=F, evaluate=F}
head(data.covid, n=5)

names(data.covid)

data.covid %>% 
  group_by(obesidad) %>% 
  summarise(media=mean(hospital))

data.covid %>% 
  group_by(hospital) %>% 
  summarise(media=mean(edad))

```

---

# Modelo de probabilidad lineal

.pull-left[
Modelamos la probabilidad de ser hospitalizado en función de la edad

¿Cómo interpretamos el coeficiente sobre edad?
]

.pull-right[
```{r tidy=T, echo=T, include=T, message=F}
res.mco <- lm(data=data.covid,
              formula = hospital~edad)
summary(res.mco)

```
]
---

# Probit

.pull-left[
Ahora estimamos un mdelo probit


```{r tidy=T, echo=T, include=T, message=F, warning=F}
# Para logit y probit usaremos solo barco y puerto

res.probit <- glm(data=data.covid,
                  formula = hospital~edad,
                  family = binomial(link = "probit"))
```

El coeficiente que obtenemos en la regresión es el coeficiente en el índice $x'\beta$.
]

.pull-right[
```{r tidy=T, echo=T, include=T, message=F}
summary(res.probit)


summary(res.probit)$coef

```
]
---

# Logit

.pull-left[
Y ahora el modelo logit

Cambiamos a logit especificando la opción *link*
]

.pull-right[
```{r tidy=T, echo=T, include=T, message=F}
res.logit <- glm(data=data.covid,
                  formula = hospital~edad,
                  family = binomial(link = "logit"))
summary(res.logit)

```
]
---

# Comparación de modelos

Un paquete útil para resumir resultados es *stargazer*

```{r tidy=T, echo=T, include=T, message=F, tidy=T}
library(stargazer)
stargazer(res.mco,res.logit,res.probit,
          type="text")
```


---

# Predicción

Los coeficientes de los modelos lineal, probit y logit no son comparables entre sí

Recuerden que, para el caso logit y probit, hay una función $F(\cdot)$ que mapea el índice en probabilidades

Eso lo logramos especificando la opción *response*

```{r tidy=T, echo=T, include=T, message=F}
data.covid <-  data.covid %>%
  mutate(p.lineal=predict(res.mco, type="response")) %>%
  mutate(p.probit=predict(res.probit, type = "response")) %>%
  mutate(p.logit=predict(res.logit, type = "response"))
```

---

# Predicción

Ya que tenemos las predicciones de las probabilidades dadas las características de cada individuo, podemos graficarlas


.pull-left[
```{r tidy=T, echo=T, include=T, message=F, fig.height=4}
data.plot <- data.covid %>%
  select(p.lineal, p.probit, p.logit, edad) %>%
  pivot_longer(cols = c("p.lineal","p.probit","p.logit"),
               names_to = "tipo",
               values_to = "efecto")
```
]

.pull-right[
```{r tidy=T, echo=T, include=T, message=F, fig.height=4}
data.plot %>% 
  ggplot()+
  geom_line(aes(x=edad,y=efecto, color=tipo)) +
  geom_hline(aes(yintercept=0), color="black", linetype="dashed")+
  geom_hline(aes(yintercept=1), color="black", linetype="dashed")
```
]


---

# Modelo multivariado


```{r tidy=T, echo=T, include=T, message=F, fig.height=4}
res.probit.2 <- glm(data=data.covid,
                  formula = hospital~edad+obesidad+fuma+sexo,
                  family = binomial(link = "probit"))
summary(res.probit.2)

```

---

# Efectos marginales

.pull-left[

Estamos interesados en $\frac{\partial P(y_i=1|x_i)}{\partial x_j}$

Como vimos antes, esta derivada depende de $x$ en el caso probit y logit

Vimos que típicamente se presenta el promedio de los efectos marginales

Calculamos $\frac{\partial P(y_i=1|x_i)}{\partial x_j}$ para cada individuo y luego obtenemos el promedio

Para esto usamos la librería *margins*
]

.pull-right[
```{r tidy=T, echo=T, include=T, eval=T, message=F, fig.height=4}
library(margins)

summary(largo.mco <- lm(data=data.covid,
                    formula = hospital~edad+obesidad+fuma+sexo))

#Margins nos da el efecto marginal promedio
summary(margins(largo.mco))

```
]

---

# Efectos marginales

Ya esperábamos que en el caso lineal los efectos marginales fueran exactamente iguales a los coeficientes

En el caso probit (y logit) no

```{r tidy=T, echo=T, include=T, message=F, fig.height=4}
summary(margins(res.probit.2))

#En realidad el que nos importa es la edad, que cambia de forma continua
summary(margins(res.probit.2, variables="edad"))
```

---

# Anatomía del efecto marginal

Vamos a calcular el promedio de los efectos marginales *a mano*

```{r tidy=T, echo=T, include=T, message=F, fig.height=4}
data.covid <- data.covid %>% 
  mutate(efecto=dnorm(predict(res.probit.2, type="link"))*summary(res.probit.2)$coef[2])
         
mean(data.covid$efecto)
   
         
summary(margins(res.probit.2, variables="edad"))

```

La ventaja del uso de *margins* es que tenemos un error estándar y la prueba de significancia

---

# Riesgo relativo

Recordemos que para el logit tenemos una interpretación de riesgo relativo de los coeficientes estimados

El riesgo relativo es $\frac{p}{1-p}=exp\{x'\beta\}$

Si $x_j$ cambia marginalmente, el riesgo relativo cambia en $exp\{x'\beta+\beta_j\}=exp\{x'\beta\}exp\{\beta_j\}$


```{r tidy=T, echo=T, include=T, message=F, fig.height=4}
res.logit.2 <- glm(data=data.covid,
                    formula = hospital~edad+obesidad+fuma+sexo,
                    family = binomial(link = "logit"))
```

---
# Riesgo relativo

Basta con exponenciar los coeficientes estimados

```{r tidy=T, echo=T, include=T, message=F, fig.height=4}
exp(cbind(OR = coef(res.logit.2), confint(res.logit.2)))
```
---
# Próxima sesión

Estudiaremos la estructura de los datos en panel

Conoceremos los modelos en datos en panel

Analizarmeos las propiedades de los estimadores de datos en panel

---

class: center, middle

Presentación creada usando el paquete [**xaringan**](https://github.com/yihui/xaringan) en R.

El *chakra* viene de [remark.js](https://remarkjs.com), [**knitr**](http://yihui.org/knitr), y [R Markdown](https://rmarkdown.rstudio.com).

Material de clase en versión preliminar.

**No reproducir, no distribuir, no citar.**